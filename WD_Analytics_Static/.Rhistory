URLencode(
paste0(pubDir, "aggRev_minutes10_stats.Rds")
)
gzcon(URLencode(
paste0(pubDir, "aggRev_minutes10_stats.Rds")
))
# - load data and provide
dataSet <- readRDS(gzcon(url(paste0(pubDir,
"aggRev_minutes10_stats.Rds"))))
View(dataSet)
# - fix missing labels
dataSet$label[nchar(dataSet$label) == 0 |
grepl("No label defined", dataSet$label)] <-
dataSet$title[nchar(dataSet$label) == 0 |
grepl("No label defined", dataSet$label)]
# - produce html
url <- paste0('https://www.wikidata.org/wiki/',
dataSet$title)
text <- paste0(dataSet$label,
" (",
dataSet$title,
")")
url <- paste0('<a href="',
url, '" target="_blank">',
text,
"</a>")
dataSet <- data.frame(Entity = url,
Revisions = dataSet$revisions,
Timestamp = dataSet$timestamp,
stringsAsFactors = F) %>%
dplyr::filter(Revisions >= 3)
View(dataSet)
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "WD_percentUsageDashboard");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp
timestamp <- rawToChar(timestamp$content)
timestamp
timestamp <- rawToChar(timestamp$content)
timestamp <- stringr::str_extract_all(timestamp, "[[:digit:]]+.+[[:digit:]]+")[[1]][3]
timestamp <- gsub("<.+", "", timestamp)
timestamp <- trimws(timestamp, which = "right")
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "WD_percentUsageDashboard");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp <- rawToChar(timestamp$content)
timestamp <- stringr::str_extract_all(timestamp, "[[:digit:]]+.+[[:digit:]]+")[[1]][3]
timestamp <- gsub("<.+", "", timestamp)
timestamp <- trimws(timestamp, which = "right")
timestamp <- paste0("Updated: ", timestamp, " UTC")
timestamp
pubDir
### --- Setup
library(DT)
library(stringr)
library(httr)
library(curl)
library(jsonlite)
pubDir <-
'https://analytics.wikimedia.org/published/datasets/wmde-analytics-engineering/Qurator/?C=S;O=D'
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "WD_percentUsageDashboard");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp <- rawToChar(timestamp$content)
timestamp <- stringr::str_extract_all(timestamp, "[[:digit:]]+.+[[:digit:]]+")[[1]][3]
timestamp <- gsub("<.+", "", timestamp)
timestamp <- trimws(timestamp, which = "right")
timestamp <- paste0("Updated: ", timestamp, " UTC")
timestamp
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "WD_percentUsageDashboard");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp <- rawToChar(timestamp$content)
timestamp
timestamp <- stringr::str_extract_all(timestamp, "[[:digit:]]+.+[[:digit:]]+")[[1]][3]
timestamp
# - get update stamp:
h <- curl::new_handle()
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "WD_percentUsageDashboard");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp <- rawToChar(timestamp$content)
timestamp <- stringr::str_extract_all(timestamp,
"[[:digit:]][[:digit:]][[:digit:]][[:digit:]]-[[:digit:]][[:digit:]]-[[:digit:]][[:digit:]]")[[1]][3]
timestamp
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "WD_percentUsageDashboard");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp <- rawToChar(timestamp$content)
timestamp
stringr::str_extract_all(timestamp,
"[[:digit:]][[:digit:]][[:digit:]][[:digit:]]-[[:digit:]][[:digit:]]-[[:digit:]][[:digit:]]")
timestamp <- stringr::str_extract_all(timestamp,
"[[:digit:]][[:digit:]][[:digit:]][[:digit:]]-[[:digit:]][[:digit:]]-[[:digit:]][[:digit:]]")[[1]][1]
timestamp <- gsub("<.+", "", timestamp)
timestamp
timestamp <- trimws(timestamp, which = "right")
timestamp <- paste0("Updated: ", timestamp, " UTC")
timestamp
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "WD_percentUsageDashboard");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp <- rawToChar(timestamp$content)
timestamp <- stringr::str_extract_all(timestamp,
"[[:digit:]]+-[[:digit:]]+-[[:digit:]]+\\s[[:digit:]]+:[[:digit:]]+")[[1]][1]
timestamp
timestamp <- trimws(timestamp, which = "both")
timestamp <- paste0("Updated: ", timestamp, " UTC")
timestamp
# - load data and provide
dataSet <- readRDS(gzcon(url(paste0(pubDir,
"aggRev_minutes10_stats.Rds"))))
# - load data and provide
dataSet <- readRDS(gzcon(url(paste0(pubDir,
"aggRev_minutes10_stats.Rds"))))
# - load data and provide
dataSet <- readRDS(gzcon(url(paste0(pubDir,
"aggRev_minutes10_stats.Rds"))))
# - load data and provide
dataSet <- readRDS(gzcon(url(paste0(pubDir,
"aggRev_minutes10_stats.Rds"))))
# - load data and provide
dataSet <- readRDS(gzcon(url(paste0(pubDir,
"aggRev_minutes10_stats.Rds"))))
# - fix missing labels
dataSet$label[nchar(dataSet$label) == 0 |
grepl("No label defined", dataSet$label)] <-
dataSet$title[nchar(dataSet$label) == 0 |
grepl("No label defined", dataSet$label)]
# - produce html
url <- paste0('https://www.wikidata.org/wiki/',
dataSet$title)
text <- paste0(dataSet$label,
" (",
dataSet$title,
")")
url <- paste0('<a href="',
url, '" target="_blank">',
text,
"</a>")
dataSet <- data.frame(Entity = url,
Revisions = dataSet$revisions,
Timestamp = dataSet$timestamp,
stringsAsFactors = F) %>%
dplyr::filter(Revisions >= 3)
View(dataSet)
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "WD_percentUsageDashboard");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp <- rawToChar(timestamp$content)
timestamp <- stringr::str_extract_all(timestamp,
"[[:digit:]]+-[[:digit:]]+-[[:digit:]]+\\s[[:digit:]]+:[[:digit:]]+")[[1]][1]
timestamp <- trimws(timestamp, which = "both")
timestamp <- paste0("Updated: ", timestamp, " UTC")
timestamp
# - load data and provide
dataSet <- readRDS(gzcon(url(paste0(pubDir,
"aggRev_hours1_stats.Rds"))))
url(paste0(pubDir,
"aggRev_hours1_stats.Rds")))
url(paste0(pubDir,
"aggRev_hours1_stats.Rds"))
# - load data and provide
dataSet <- readRDS(gzcon(URLencode(paste0(pubDir,
"aggRev_hours1_stats.Rds"))))
# - load data and provide
dataSet <- readRDS(gzcon(url(paste0(pubDir,
"aggRev_hours1_stats.Rds"))))
>url
?url
# - load data and provide
dataSet <- readRDS(gzcon(url(paste0(pubDir,
"aggRev_hours1_stats.Rds"))))
aggRev_hours1_stats_test <- readRDS("~/Downloads/aggRev_hours1_stats.Rds")
View(aggRev_hours1_stats_test)
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
pubDir <-
'https://analytics.wikimedia.org/published/datasets/wmde-analytics-engineering/Qurator/'
rm(list = ls())
gc()
### --- Setup
library(DT)
library(stringr)
library(httr)
library(curl)
library(jsonlite)
### --- Setup
library(DT)
library(stringr)
library(httr)
library(curl)
library(jsonlite)
pubDir <-
'https://analytics.wikimedia.org/published/datasets/wmde-analytics-engineering/Qurator/'
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "WD_percentUsageDashboard");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp <- rawToChar(timestamp$content)
timestamp <- stringr::str_extract_all(timestamp,
"[[:digit:]]+-[[:digit:]]+-[[:digit:]]+\\s[[:digit:]]+:[[:digit:]]+")[[1]][1]
timestamp <- trimws(timestamp, which = "both")
timestamp <- paste0("Updated: ", timestamp, " UTC")
timestamp
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "WD_percentUsageDashboard");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp <- rawToChar(timestamp$content)
timestamp <- stringr::str_extract_all(timestamp,
"[[:digit:]]+-[[:digit:]]+-[[:digit:]]+\\s[[:digit:]]+:[[:digit:]]+")[[1]][1]
timestamp <- trimws(timestamp, which = "both")
timestamp <- paste0("Updated: ", timestamp, " UTC")
timestamp
timestamp
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "QURATOR_CurrentEvents");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp <- rawToChar(timestamp$content)
timestamp <- stringr::str_extract_all(timestamp,
"[[:digit:]]+-[[:digit:]]+-[[:digit:]]+\\s[[:digit:]]+:[[:digit:]]+")[[1]][1]
timestamp <- trimws(timestamp, which = "both")
timestamp <- paste0("Updated: ", timestamp, " UTC")
timestamp
pubDir <-
'https://analytics.wikimedia.org/published/datasets/wmde-analytics-engineering/Qurator'
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "QURATOR_CurrentEvents");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp <- rawToChar(timestamp$content)
timestamp <- stringr::str_extract_all(timestamp,
"[[:digit:]]+-[[:digit:]]+-[[:digit:]]+\\s[[:digit:]]+:[[:digit:]]+")[[1]][1]
timestamp <- trimws(timestamp, which = "both")
timestamp <- paste0("Updated: ", timestamp, " UTC")
timestamp
# - load data and provide
dataSet <- readRDS(gzcon(url(paste0(pubDir,
"aggRev_minutes10_stats.Rds"))))
pubDir <-
'https://analytics.wikimedia.org/published/datasets/wmde-analytics-engineering/Qurator/'
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "QURATOR_CurrentEvents");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp <- rawToChar(timestamp$content)
timestamp <- stringr::str_extract_all(timestamp,
"[[:digit:]]+-[[:digit:]]+-[[:digit:]]+\\s[[:digit:]]+:[[:digit:]]+")[[1]][1]
timestamp <- trimws(timestamp, which = "both")
timestamp <- paste0("Updated: ", timestamp, " UTC")
timestamp
# - load data and provide
dataSet <- readRDS(gzcon(url(paste0(pubDir,
"aggRev_minutes10_stats.Rds"))))
View(dataSet)
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
# - get update stamp:
h <- curl::new_handle()
curl::handle_setopt(h,
copypostfields = "QURATOR_CurrentEvents");
curl::handle_setheaders(h,
"Cache-Control" = "no-cache"
)
timestamp <- curl::curl_fetch_memory(URLencode(pubDir))
timestamp <- rawToChar(timestamp$content)
timestamp <- stringr::str_extract_all(timestamp,
"[[:digit:]]+-[[:digit:]]+-[[:digit:]]+\\s[[:digit:]]+:[[:digit:]]+")[[1]][1]
timestamp <- trimws(timestamp, which = "both")
timestamp <- paste0("Updated: ", timestamp, " UTC")
timestamp
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
dataSet$label
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CurrentEvents/dashboard')
rmarkdown::render_site()
runApp('~/Work/___DataKolektiv/Projects/WikimediaDEU/_WMDE_Projects/Qurator/CuriousFacts/_dashboard')
### --- Setup
library(data.table)
library(dplyr)
library(tidyr)
library(httr)
library(curl)
library(XML)
library(stringr)
library(jsonlite)
### --- shinyServer
shinyServer(function(input, output, session) {
endPointURL <-
'https://query.wikidata.org/bigdata/namespace/wdq/sparql?query='
### --- Data
# - get current Qurator Curious Facts:
withProgress(message = 'The Dashboard is loading data.',
detail = "Please be patient.", value = 0, {
dataM1 <- data.table::fread('_data/dataM1_processed.csv', header = T)
infoM1 <- data.table::fread('_data/infoM1.csv', header = T)
infoM1$V1 <- NULL
incProgress(amount = .25)
dataM2 <- data.table::fread('_data/dataM2_processed.csv', header = T)
infoM2 <- data.table::fread('_data/infoM2.csv', header = T)
infoM2$V1 <- NULL
incProgress(amount = .25)
dataM3 <- data.table::fread('_data/dataM3_processed.csv', header = T)
infoM3 <- data.table::fread('_data/infoM3.csv', header = T)
infoM3$V1 <- NULL
# - lists
incProgress(amount = .25)
dataSet <- list(dataM1, dataM2, dataM3)
infoSet <- list(infoM1, infoM2, infoM3)
incProgress(amount = .25)
# - clean
rm(list = c('dataM1', 'dataM2', 'dataM3'))
})
### --- First Fact
# - select dataset
cD <- sample(1:length(dataSet), 1)
dS <- dplyr::select(dataSet[[cD]],
explanation,
establishedOn)
iS <- infoSet[[cD]]
# - select random fact
rf <- sample(1:dim(dS)[1], 1)
fact <- dS$explanation[rf]
establishedOn <- dS$establishedOn[rf]
### --- First Fact
output$fact <- renderText({
# - format output
out <- paste0('<p style="font-size:120%;"align="left">',
fact, '</p><br>',
'<p style="font-size:120%;"align="left"> This fact was established on: <b>',
establishedOn, "</b>, and is based on the <b>",
iS$wdDumpSnapshot, "</b> snapshot in hdfs of the Wikidata JSON dump.")
return(out)
})
### --- First Image
output$factImage <- renderText({
item <- dataSet[[cD]]$item[rf]
query <- paste0('SELECT ?image {wd:',
item,
' wdt:P18 ?image .}')
res <- GET(url = paste0(endPointURL, URLencode(query)))
if (res$status_code == 200) {
src <- fromJSON(rawToChar(res$content), simplifyDataFrame = T)
src <- src$results$bindings$image$value
srcimg <- tryCatch({tail(strsplit(src, split = "/")[[1]], 1)},
error = function(condition) {
NULL
})
if(is.null(srcimg)) {
return(
'<p style="font-size:150%;"align="left"><b>OH NO - THERE IS NO IMAGE FOR THIS THING IN WIKIDATA.</b></p>'
)
}
srcimg <- paste0("https://magnus-toolserver.toolforge.org/commonsapi.php?image=",
srcimg,
"&thumbwidth=300")
srcimg <- GET(srcimg)
srcimg <- xmlToList(xmlParse(srcimg))
if (length(!is.null(srcimg$file$urls$thumbnail)) > 0) {
return(paste0('<img src="',URLencode(srcimg$file$urls$thumbnail),'">'))
} else {
return(
'<p style="font-size:150%;"align="left"><b>OH NO - THERE IS NO IMAGE FOR THIS THING IN WIKIDATA.</b></p>'
)
}
} else {
return(
'<p style="font-size:150%;"align="left"><b>OH NO - WE CANNNOT OBTAIN THE IMAGE NOF THIS ITEM.</b></p>'
)
}
})
### --- Present Facts Loop
observeEvent(input$button, {
# - select dataset
cD <- sample(1:length(dataSet), 1)
dS <- dplyr::select(dataSet[[cD]],
explanation,
establishedOn)
iS <- infoSet[[cD]]
# - select random fact
rf <- sample(1:dim(dS)[1], 1)
fact <- dS$explanation[rf]
establishedOn <- dS$establishedOn[rf]
output$fact <- renderText({
# - format output
out <- paste0('<p style="font-size:120%;"align="left">',
fact, '</p><hr>',
'<p style="font-size:120%;"align="left"> This fact was established on: <b>',
establishedOn, "</b>, and is based on the <b>",
iS$wdDumpSnapshot, "</b> snapshot in hdfs of the Wikidata JSON dump.")
return(out)
})
output$factImage <- renderText({
item <- dataSet[[cD]]$item[rf]
query <- paste0('SELECT ?image {wd:',
item,
' wdt:P18 ?image .}')
res <- GET(url = paste0(endPointURL, URLencode(query)))
if (res$status_code == 200) {
src <- fromJSON(rawToChar(res$content), simplifyDataFrame = T)
src <- src$results$bindings$image$value
srcimg <- tryCatch({tail(strsplit(src, split = "/")[[1]], 1)},
error = function(condition) {
NULL
})
if(is.null(srcimg)) {
return(
'<p style="font-size:150%;"align="left"><b>OH NO - THERE IS NO IMAGE FOR THIS THING IN WIKIDATA.</b></p>'
)
}
srcimg <- paste0("https://magnus-toolserver.toolforge.org/commonsapi.php?image=",
srcimg,
"&thumbwidth=300")
srcimg <- GET(srcimg)
srcimg <- xmlToList(xmlParse(srcimg))
if (length(!is.null(srcimg$file$urls$thumbnail)) > 0) {
return(paste0('<img src="',URLencode(srcimg$file$urls$thumbnail),'">'))
} else {
return(
'<p style="font-size:150%;"align="left"><b>OH NO - THERE IS NO IMAGE FOR THIS THING IN WIKIDATA.</b></p>'
)
}
} else {
return(
'<p style="font-size:150%;"align="left"><b>OH NO - WE CANNNOT OBTAIN THE IMAGE NOF THIS ITEM.</b></p>'
)
}
})
})
})
### --- Setup
library(data.table)
library(dplyr)
library(tidyr)
library(httr)
library(curl)
library(XML)
library(stringr)
library(jsonlite)
endPointURL <-
'https://query.wikidata.org/bigdata/namespace/wdq/sparql?query='
dataM1 <- data.table::fread('_data/dataM1_processed.csv', header = T)
infoM1 <- data.table::fread('_data/infoM1.csv', header = T)
